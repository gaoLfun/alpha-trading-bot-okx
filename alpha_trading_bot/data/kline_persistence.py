"""
Kçº¿æ•°æ®æŒä¹…åŒ–ç®¡ç†æ¨¡å—

åŠŸèƒ½ï¼š
1. å°† K çº¿æ•°æ®ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼ˆJSON æ ¼å¼ï¼‰
2. å¯åŠ¨æ—¶ä»æœ¬åœ°åŠ è½½å†å²æ•°æ®
3. å®ç°å¢é‡æ›´æ–°ï¼Œåªè·å–æ–° K çº¿
4. è‡ªåŠ¨ç»´æŠ¤æ•°æ®æ–‡ä»¶ï¼Œæ¸…ç†è¿‡æœŸæ•°æ®

ä½œè€…: Alpha Trading Bot
æ—¥æœŸ: 2026-01-23
"""

import asyncio
import json
import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import time

logger = logging.getLogger(__name__)

# æ•°æ®ç›®å½•
DATA_DIR = Path(__file__).parent.parent.parent / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)


@dataclass
class OHLCVData:
    """Kçº¿æ•°æ®"""

    timestamp: int  # æ—¶é—´æˆ³ (æ¯«ç§’)
    open_time: str  # å¼€æ”¾æ—¶é—´å­—ç¬¦ä¸²
    open_price: float  # å¼€ç›˜ä»·
    high_price: float  # æœ€é«˜ä»·
    low_price: float  # æœ€ä½ä»·
    close_price: float  # æ”¶ç›˜ä»·
    volume: float  # æˆäº¤é‡

    def to_list(self) -> List:
        """è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆä¿æŒtimestampä¸ºæ•´æ•°ï¼Œopen_timeä¸ºå­—ç¬¦ä¸²ï¼‰"""
        return [
            self.timestamp,  # ä¿æŒæ•´æ•°æ—¶é—´æˆ³
            self.open_time,  # å­—ç¬¦ä¸²æ ¼å¼
            self.open_price,
            self.high_price,
            self.low_price,
            self.close_price,
            self.volume,
        ]

    @classmethod
    def from_list(cls, data: List) -> "OHLCVData":
        """ä»åˆ—è¡¨åˆ›å»ºï¼ˆå…¼å®¹æ•´æ•°æ—¶é—´æˆ³å’Œå­—ç¬¦ä¸²open_timeï¼‰"""
        return cls(
            timestamp=int(data[0]),  # ç¡®ä¿æ˜¯æ•´æ•°
            open_time=str(data[1]),  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
            open_price=float(data[2]),
            high_price=float(data[3]),
            low_price=float(data[4]),
            close_price=float(data[5]),
            volume=float(data[6]),
        )

    @classmethod
    def from_ccxt(cls, candle: List) -> "OHLCVData":
        """ä» CCXT æ ¼å¼åˆ›å»º"""
        return cls(
            timestamp=candle[0],
            open_time=datetime.fromtimestamp(candle[0] / 1000).strftime(
                "%Y-%m-%d %H:%M:%S"
            ),
            open_price=float(candle[1]),
            high_price=float(candle[2]),
            low_price=float(candle[3]),
            close_price=float(candle[4]),
            volume=float(candle[5]),
        )


@dataclass
class KLineFileMetadata:
    """Kçº¿æ–‡ä»¶å…ƒæ•°æ®"""

    symbol: str
    timeframe: str
    last_update: str  # æœ€åæ›´æ–°æ—¶é—´
    last_timestamp: int  # æœ€åä¸€æ¡ K çº¿çš„æ—¶é—´æˆ³
    count: int  # K çº¿æ•°é‡
    file_size: int  # æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰

    def to_dict(self) -> Dict:
        return asdict(self)


class KLinePersistenceManager:
    """Kçº¿æ•°æ®æŒä¹…åŒ–ç®¡ç†å™¨"""

    def __init__(self, data_dir: Path = None):
        """
        åˆå§‹åŒ– K çº¿æŒä¹…åŒ–ç®¡ç†å™¨

        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
        """
        self.data_dir = data_dir or DATA_DIR
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # æ–‡ä»¶ç¼“å­˜ (symbol:timeframe -> file_path)
        self._file_cache: Dict[str, Path] = {}

        # å†…å­˜ç¼“å­˜ (symbol:timeframe -> List[OHLCVData])
        self._memory_cache: Dict[str, List[OHLCVData]] = {}

        # æœ€å¤§ä¿å­˜å¤©æ•°
        self.max_days = 30  # ä¿å­˜ 30 å¤©å†å²æ•°æ®

        # æ¯ä¸ªæ—¶é—´æ®µçš„ K çº¿æ•°é‡ä¸Šé™
        self.max_candles = {
            "5m": 30 * 24 * 12,  # 30å¤© * 24å°æ—¶ * 12 (5åˆ†é’Ÿ)
            "15m": 30 * 24 * 4,  # 30å¤© * 24å°æ—¶ * 4 (15åˆ†é’Ÿ)
            "1h": 30 * 24,  # 30å¤© * 24å°æ—¶
            "4h": 30 * 6,  # 30å¤© * 6 (4å°æ—¶)
            "1d": 30,  # 30å¤©
        }

        logger.info(f"KLinePersistenceManager åˆå§‹åŒ–å®Œæˆï¼Œæ•°æ®ç›®å½•: {self.data_dir}")

    def _get_file_path(self, symbol: str, timeframe: str) -> Path:
        """
        è·å– K çº¿æ•°æ®æ–‡ä»¶è·¯å¾„

        Args:
            symbol: äº¤æ˜“å¯¹ (å¦‚ BTC/USDT:USDT)
            timeframe: æ—¶é—´å‘¨æœŸ (å¦‚ 5m, 15m, 1h)

        Returns:
            æ–‡ä»¶è·¯å¾„
        """
        cache_key = f"{symbol}:{timeframe}"
        if cache_key in self._file_cache:
            return self._file_cache[cache_key]

        # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        safe_symbol = symbol.replace("/", "_").replace(":", "_")
        filename = f"kline_{safe_symbol}_{timeframe}.json"
        file_path = self.data_dir / filename

        self._file_cache[cache_key] = file_path
        return file_path

    def save_klines(self, symbol: str, timeframe: str, klines: List[List]) -> bool:
        """
        ä¿å­˜ K çº¿æ•°æ®åˆ°æ–‡ä»¶

        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
            klines: K çº¿æ•°æ®åˆ—è¡¨ï¼ˆæ”¯æŒ CCXT æ ¼å¼æˆ–å·²è½¬æ¢æ ¼å¼ï¼‰

        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            file_path = self._get_file_path(symbol, timeframe)

            # è½¬æ¢æ•°æ®ï¼ˆå…¼å®¹ CCXT æ ¼å¼å’Œå·²ä¿å­˜æ ¼å¼ï¼‰
            ohlcv_data = []
            for k in klines:
                if isinstance(k[0], int) and isinstance(k[1], str):
                    # å·²ç»æ˜¯ OHLCVData æ ¼å¼ï¼ˆä»æ–‡ä»¶åŠ è½½çš„ï¼‰
                    ohlcv_data.append(k)
                else:
                    # CCXT æ ¼å¼ï¼Œéœ€è¦è½¬æ¢
                    ohlcv_data.append(OHLCVData.from_ccxt(k).to_list())

            if not ohlcv_data:
                logger.warning(f"æ²¡æœ‰ K çº¿æ•°æ®éœ€è¦ä¿å­˜: {symbol} {timeframe}")
                return False

            # æŒ‰æ—¶é—´æˆ³æ’åº
            ohlcv_data.sort(key=lambda x: x[0])

            # é™åˆ¶æ•°é‡ï¼Œä¿ç•™æœ€è¿‘çš„ max_candles æ¡
            max_count = self.max_candles.get(timeframe, 2000)
            if len(ohlcv_data) > max_count:
                ohlcv_data = ohlcv_data[-max_count:]
                logger.info(f"å·²æˆªå–æœ€è¿‘ {max_count} æ ¹ K çº¿")

            # æ„å»ºæ–‡ä»¶æ•°æ®
            file_data = {
                "metadata": {
                    "symbol": symbol,
                    "timeframe": timeframe,
                    "last_update": datetime.now().isoformat(),
                    "last_timestamp": ohlcv_data[-1][0],
                    "count": len(ohlcv_data),
                    "version": "1.0",
                },
                "klines": ohlcv_data,
            }

            # å†™å…¥æ–‡ä»¶
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(file_data, f, indent=2, ensure_ascii=False)

            # æ¸…ç†å†…å­˜ç¼“å­˜
            cache_key = f"{symbol}:{timeframe}"
            if cache_key in self._memory_cache:
                del self._memory_cache[cache_key]

            logger.info(
                f"âœ… Kçº¿æ•°æ®å·²ä¿å­˜: {symbol} {timeframe} - "
                f"{len(ohlcv_data)} æ ¹, æ–‡ä»¶: {file_path.name}"
            )
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜ K çº¿æ•°æ®å¤±è´¥: {symbol} {timeframe} - {e}")
            return False

    def load_klines(
        self, symbol: str, timeframe: str
    ) -> Tuple[List[List], Optional[KLineFileMetadata]]:
        """
        ä»æ–‡ä»¶åŠ è½½ K çº¿æ•°æ®

        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ

        Returns:
            (Kçº¿æ•°æ®åˆ—è¡¨, å…ƒæ•°æ®) - å¦‚æœæ²¡æœ‰æœ¬åœ°æ•°æ®åˆ™è¿”å›ç©ºåˆ—è¡¨
        """
        try:
            file_path = self._get_file_path(symbol, timeframe)

            if not file_path.exists():
                logger.info(f"æœ¬åœ° K çº¿æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return [], None

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            if file_size == 0:
                logger.warning(f"æœ¬åœ° K çº¿æ•°æ®æ–‡ä»¶ä¸ºç©º: {file_path}")
                return [], None

            with open(file_path, "r", encoding="utf-8") as f:
                file_data = json.load(f)

            # è§£æå…ƒæ•°æ®
            metadata_dict = file_data.get("metadata", {})
            metadata = KLineFileMetadata(
                symbol=metadata_dict.get("symbol", symbol),
                timeframe=metadata_dict.get("timeframe", timeframe),
                last_update=metadata_dict.get("last_update", ""),
                last_timestamp=metadata_dict.get("last_timestamp", 0),
                count=metadata_dict.get("count", 0),
                file_size=file_size,
            )

            # è§£æ K çº¿æ•°æ®
            klines = file_data.get("klines", [])

            # æ›´æ–°å†…å­˜ç¼“å­˜
            cache_key = f"{symbol}:{timeframe}"
            self._memory_cache[cache_key] = klines

            logger.info(
                f"ğŸ“‚ å·²åŠ è½½æœ¬åœ° K çº¿æ•°æ®: {symbol} {timeframe} - "
                f"{len(klines)} æ ¹, æ›´æ–°æ—¶é—´: {metadata.last_update}"
            )

            return klines, metadata

        except json.JSONDecodeError as e:
            logger.error(f"è§£æ K çº¿æ•°æ®æ–‡ä»¶å¤±è´¥: {file_path} - {e}")
            return [], None
        except Exception as e:
            logger.error(f"åŠ è½½ K çº¿æ•°æ®å¤±è´¥: {symbol} {timeframe} - {e}")
            return [], None

    def get_klines(
        self, symbol: str, timeframe: str, limit: int = None, since: int = None
    ) -> Tuple[List[List], bool]:
        """
        è·å– K çº¿æ•°æ®ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰

        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
            limit: é™åˆ¶æ•°é‡ (é»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶æˆ– max_candles)
            since: èµ·å§‹æ—¶é—´æˆ³ (æ¯«ç§’)

        Returns:
            (Kçº¿æ•°æ®åˆ—è¡¨, æ˜¯å¦ä¸ºå¢é‡æ›´æ–°)
        """
        cache_key = f"{symbol}:{timeframe}"

        # 1. å…ˆå°è¯•ä»æœ¬åœ°åŠ è½½
        local_klines, metadata = self.load_klines(symbol, timeframe)

        # 2. åˆ¤æ–­æ˜¯å¦éœ€è¦è·å–æ–°æ•°æ®
        need_fetch = True
        current_timestamp = int(time.time() * 1000)

        if local_klines and metadata:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            timeframe_ms = self._timeframe_to_ms(timeframe)
            since_timestamp = since or (
                current_timestamp - 7 * 24 * 60 * 60 * 1000
            )  # é»˜è®¤ 7 å¤©

            # æ£€æŸ¥æœ¬åœ°æ•°æ®æ˜¯å¦è¶³å¤Ÿ
            oldest_local_timestamp = local_klines[0][0] if local_klines else 0

            if since and since >= oldest_local_timestamp:
                # åªè¯·æ±‚æŒ‡å®šæ—¶é—´ä¹‹åçš„æ•°æ®
                need_fetch = True
            elif since is None and len(local_klines) >= (
                limit or self.max_candles.get(timeframe, 2000)
            ):
                # æœ¬åœ°æ•°æ®è¶³å¤Ÿï¼Œä¸”æ˜¯æœ€æ–°çš„
                need_fetch = False
            else:
                # æ£€æŸ¥æœ¬åœ°æ•°æ®æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡ 5 åˆ†é’Ÿï¼‰
                last_update = datetime.fromisoformat(metadata.last_update)
                if (datetime.now() - last_update).total_seconds() < 300:
                    need_fetch = False

        # 3. å¦‚æœéœ€è¦è·å–æ–°æ•°æ®
        if need_fetch:
            # è¿™é‡Œè¿”å›æœ¬åœ°æ•°æ® + éœ€è¦è·å–çš„èµ·å§‹æ—¶é—´æˆ³
            # å®é™…è·å–ç”±è°ƒç”¨æ–¹å®Œæˆ
            pass

        # è¿‡æ»¤å’Œæˆªå–æ•°æ®
        result_klines = local_klines

        if since:
            result_klines = [k for k in result_klines if k[0] >= since]

        if limit:
            result_klines = result_klines[-limit:]

        return result_klines, need_fetch

    def merge_klines(
        self, symbol: str, timeframe: str, new_klines: List[List]
    ) -> List[List]:
        """
        åˆå¹¶æ–°æ—§ K çº¿æ•°æ®

        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
            new_klines: æ–°è·å–çš„ K çº¿æ•°æ®

        Returns:
            åˆå¹¶åçš„ K çº¿æ•°æ®
        """
        # åŠ è½½æœ¬åœ°æ•°æ®
        local_klines, _ = self.load_klines(symbol, timeframe)

        if not local_klines:
            # æ²¡æœ‰æœ¬åœ°æ•°æ®ï¼Œç›´æ¥ä¿å­˜
            self.save_klines(symbol, timeframe, new_klines)
            return new_klines

        # åˆå¹¶æ•°æ®
        all_klines = {}

        # æ·»åŠ æœ¬åœ°æ•°æ®
        for k in local_klines:
            all_klines[k[0]] = k

        # æ·»åŠ æ–°æ•°æ®
        for k in new_klines:
            all_klines[k[0]] = k

        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
        merged = list(all_klines.values())
        merged.sort(key=lambda x: x[0])

        # é™åˆ¶æ•°é‡
        max_count = self.max_candles.get(timeframe, 2000)
        if len(merged) > max_count:
            merged = merged[-max_count:]

        return merged

    def update_klines(
        self, symbol: str, timeframe: str, new_klines: List[List]
    ) -> bool:
        """
        æ›´æ–° K çº¿æ•°æ®ï¼ˆå¢é‡æ›´æ–°ï¼‰

        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
            new_klines: æ–°è·å–çš„ K çº¿æ•°æ®

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        # åˆå¹¶æ•°æ®
        merged_klines = self.merge_klines(symbol, timeframe, new_klines)

        # ä¿å­˜
        return self.save_klines(symbol, timeframe, merged_klines)

    def _timeframe_to_ms(self, timeframe: str) -> int:
        """å°†æ—¶é—´å‘¨æœŸè½¬æ¢ä¸ºæ¯«ç§’"""
        unit = timeframe[-1]
        value = int(timeframe[:-1])

        multipliers = {
            "m": 60 * 1000,  # åˆ†é’Ÿ
            "h": 60 * 60 * 1000,  # å°æ—¶
            "d": 24 * 60 * 60 * 1000,  # å¤©
            "w": 7 * 24 * 60 * 60 * 1000,  # å‘¨
        }

        return value * multipliers.get(unit, 60 * 1000)

    def get_data_info(self, symbol: str, timeframe: str) -> Dict:
        """
        è·å– K çº¿æ•°æ®ä¿¡æ¯

        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ

        Returns:
            æ•°æ®ä¿¡æ¯å­—å…¸
        """
        file_path = self._get_file_path(symbol, timeframe)

        if not file_path.exists():
            return {
                "exists": False,
                "symbol": symbol,
                "timeframe": timeframe,
                "count": 0,
                "file_path": str(file_path),
            }

        # åŠ è½½å…ƒæ•°æ®
        _, metadata = self.load_klines(symbol, timeframe)

        return {
            "exists": True,
            "symbol": symbol,
            "timeframe": timeframe,
            "count": metadata.count if metadata else 0,
            "last_update": metadata.last_update if metadata else "",
            "last_timestamp": metadata.last_timestamp if metadata else 0,
            "file_path": str(file_path),
            "file_size": metadata.file_size if metadata else 0,
        }

    def cleanup_old_data(self, symbol: str = None, timeframe: str = None):
        """
        æ¸…ç†è¿‡æœŸæ•°æ®

        Args:
            symbol: äº¤æ˜“å¯¹ (None åˆ™æ¸…ç†æ‰€æœ‰)
            timeframe: æ—¶é—´å‘¨æœŸ (None åˆ™æ¸…ç†æ‰€æœ‰)
        """
        if symbol and timeframe:
            # æ¸…ç†å•ä¸ªæ–‡ä»¶
            file_path = self._get_file_path(symbol, timeframe)
            if file_path.exists():
                # é‡æ–°åŠ è½½å¹¶ä¿å­˜ï¼ˆä¼šè§¦å‘æˆªæ–­ï¼‰
                klines, _ = self.load_klines(symbol, timeframe)
                if klines:
                    self.save_klines(symbol, timeframe, klines)
                    logger.info(f"å·²æ¸…ç†æ•°æ®: {symbol} {timeframe}")
        else:
            # æ¸…ç†æ‰€æœ‰æ–‡ä»¶
            for file_path in self.data_dir.glob("kline_*.json"):
                try:
                    # æå– symbol å’Œ timeframe
                    parts = file_path.stem.replace("kline_", "").split("_")
                    if len(parts) >= 3:
                        s = parts[0] + "/" + parts[1].replace("USDT", ":USDT")
                        t = "_".join(parts[2:])
                        self.cleanup_old_data(s, t)
                except Exception as e:
                    logger.warning(f"æ¸…ç†æ•°æ®æ–‡ä»¶å¤±è´¥: {file_path} - {e}")

    def clear_cache(self):
        """æ¸…ç†å†…å­˜ç¼“å­˜"""
        self._memory_cache.clear()
        logger.info("Kçº¿æ•°æ®å†…å­˜ç¼“å­˜å·²æ¸…ç†")


# å…¨å±€å®ä¾‹
kline_persistence_manager = KLinePersistenceManager()


def get_kline_manager() -> KLinePersistenceManager:
    """è·å– K çº¿æŒä¹…åŒ–ç®¡ç†å™¨å®ä¾‹"""
    return kline_persistence_manager
