"""
AIç¼“å­˜ç®¡ç†å™¨ - ä¸“é—¨è´Ÿè´£ä¿¡å·çš„ç¼“å­˜å’Œæ™ºèƒ½å¤±æ•ˆç®¡ç†
"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

from .dynamic_cache import DynamicCacheManager, cache_manager
from .cache_monitor import cache_monitor

logger = logging.getLogger(__name__)


class AICacheManager:
    """AIç¼“å­˜ç®¡ç†å™¨ - ä¸“é—¨è´Ÿè´£ç¼“å­˜é€»è¾‘"""

    def __init__(self, config=None):
        self.cache: Dict[str, Any] = {}
        self.dynamic_cache = cache_manager
        self.enable_dynamic_cache = True
        self.cache_duration = 900  # é»˜è®¤15åˆ†é’Ÿ

        if config:
            self.enable_dynamic_cache = getattr(config, "enable_dynamic_cache", True)
            self.cache_duration = getattr(config, "cache_duration", 900)
            # åŒæ­¥åŠ¨æ€ç¼“å­˜é…ç½®
            self.dynamic_cache.config.base_duration = self.cache_duration

    def generate_cache_key(self, market_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        if self.enable_dynamic_cache:
            return self.dynamic_cache.generate_cache_key_v2(market_data)
        else:
            return self._generate_simple_cache_key(market_data)

    def _generate_simple_cache_key(self, market_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆç®€å•ç¼“å­˜é”®ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰"""
        price = market_data.get("price", 0)
        rsi = market_data.get("technical_data", {}).get("rsi", 50)
        macd = market_data.get("technical_data", {}).get("macd_histogram", 0)

        # ä»·æ ¼å’Œä¸»è¦æŠ€æœ¯æŒ‡æ ‡çš„ç»„åˆ
        price_bucket = int(price / 100) * 100  # 100ç¾Žå…ƒä»·æ ¼åŒºé—´
        rsi_bucket = int(rsi / 5) * 5  # 5ç‚¹RSIåŒºé—´
        macd_sign = "pos" if macd > 0 else "neg" if macd < 0 else "zero"

        return f"ai_signal_{price_bucket}_{rsi_bucket}_{macd_sign}"

    def check_cache(
        self, market_data: Dict[str, Any]
    ) -> tuple[Optional[List[Dict[str, Any]]], Optional[Dict[str, Any]]]:
        """
        æ£€æŸ¥ç¼“å­˜

        Returns:
            tuple: (cached_signals, cache_metadata)
        """
        try:
            cache_key = self.generate_cache_key(market_data)

            if cache_key not in self.cache:
                self.dynamic_cache.record_cache_miss()
                cache_monitor.record_miss(cache_key)
                return None, None

            cached_result = self.cache[cache_key]

            # ç¡®å®šç¼“å­˜æŒç»­æ—¶é—´
            if self.enable_dynamic_cache:
                atr_percentage = market_data.get("atr_percentage", 0)
                dynamic_duration = self.dynamic_cache.get_dynamic_cache_duration(
                    atr_percentage
                )
                logger.info(
                    f"ðŸ”„ ä½¿ç”¨åŠ¨æ€ç¼“å­˜ç³»ç»Ÿ - ATR: {atr_percentage:.2f}%, ç¼“å­˜æ—¶é—´: {dynamic_duration}ç§’"
                )
            else:
                dynamic_duration = self.cache_duration

            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            cache_age = (datetime.now() - cached_result["timestamp"]).total_seconds()
            if cache_age >= dynamic_duration:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self.cache[cache_key]
                self.dynamic_cache.record_cache_miss()
                cache_monitor.record_miss(cache_key)
                logger.info(f"ç¼“å­˜å·²è¿‡æœŸï¼Œåˆ é™¤ç¼“å­˜é”®: {cache_key}")
                return None, None

            # æ™ºèƒ½å¤±æ•ˆæ£€æµ‹
            if self.enable_dynamic_cache:
                should_invalidate, reason = self.dynamic_cache.should_invalidate_cache(
                    market_data, cached_result.get("market_snapshot", {})
                )
                if should_invalidate:
                    logger.info(f"ðŸ”„ æ™ºèƒ½ç¼“å­˜å¤±æ•ˆ: {reason}")
                    del self.cache[cache_key]
                    self.dynamic_cache.record_cache_eviction()
                    cache_monitor.record_eviction(cache_key, reason)
                    return None, None

            # ç¼“å­˜æœ‰æ•ˆ
            self.dynamic_cache.record_cache_hit()
            cache_monitor.record_hit(cache_key, cache_age)

            # è¿”å›žç¼“å­˜çš„ä¿¡å·å’Œå…ƒæ•°æ®
            signals = cached_result.get("signals", [])
            for signal in signals:
                signal["_from_cache"] = True  # æ ‡è®°ä¸ºç¼“å­˜ä¿¡å·

            metadata = {
                "success_count": cached_result.get("success_count", 0),
                "fail_count": cached_result.get("fail_count", 0),
                "success_providers": cached_result.get("success_providers", []),
                "cache_age": cache_age,
                "from_cache": True,
            }

            return signals, metadata

        except Exception as e:
            logger.error(f"æ£€æŸ¥ç¼“å­˜å¤±è´¥: {e}")
            return None, None

    def save_to_cache(
        self,
        signals: List[Dict[str, Any]],
        market_data: Dict[str, Any],
        success_count: int = 0,
        fail_count: int = 0,
        success_providers: List[str] = None,
    ) -> None:
        """ä¿å­˜ä¿¡å·åˆ°ç¼“å­˜"""
        try:
            cache_key = self.generate_cache_key(market_data)

            cache_data = {
                "signals": signals,
                "success_count": success_count,
                "fail_count": fail_count,
                "success_providers": success_providers or [],
                "timestamp": datetime.now(),
            }

            # å¦‚æžœä½¿ç”¨åŠ¨æ€ç¼“å­˜ï¼Œä¿å­˜å¸‚åœºå¿«ç…§ç”¨äºŽæ™ºèƒ½å¤±æ•ˆæ£€æµ‹
            if self.enable_dynamic_cache:
                cache_data["market_snapshot"] = {
                    "price": market_data.get("price", 0),
                    "volume": market_data.get("volume", 0),
                    "atr": market_data.get("atr", 0),
                    "atr_percentage": market_data.get("atr_percentage", 0),
                    "technical_data": market_data.get("technical_data", {}),
                }

            self.cache[cache_key] = cache_data
            logger.info(f"ä¿¡å·å·²ä¿å­˜åˆ°ç¼“å­˜: {cache_key}")

        except Exception as e:
            logger.error(f"ä¿å­˜åˆ°ç¼“å­˜å¤±è´¥: {e}")

    def log_cache_stats(self, metadata: Dict[str, Any]) -> None:
        """è®°å½•ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not metadata or not metadata.get("from_cache"):
            return

        success_count = metadata.get("success_count", 0)
        fail_count = metadata.get("fail_count", 0)
        success_providers = metadata.get("success_providers", [])
        cache_age = metadata.get("cache_age", 0)

        total = success_count + fail_count
        logger.info(
            f"ðŸ“Š å¤šAIä¿¡å·èŽ·å–ç»Ÿè®¡: æˆåŠŸ={success_count}, å¤±è´¥={fail_count}, æ€»è®¡={total}"
        )
        logger.info(
            f"âœ… æˆåŠŸæä¾›å•†: {success_providers if success_providers else 'æ— '}"
        )
        logger.info(f"ðŸ• ç¼“å­˜å¹´é¾„: {cache_age:.1f}ç§’")

    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        cache_count = len(self.cache)
        self.cache.clear()
        logger.info(f"å·²æ¸…ç©ºç¼“å­˜ï¼Œåˆ é™¤äº† {cache_count} ä¸ªç¼“å­˜é¡¹")

    def get_cache_info(self) -> Dict[str, Any]:
        """èŽ·å–ç¼“å­˜ä¿¡æ¯"""
        total_count = len(self.cache)
        expired_count = sum(
            1
            for item in self.cache.values()
            if (datetime.now() - item["timestamp"]).total_seconds()
            > self.cache_duration
        )

        return {
            "total_cache_items": total_count,
            "expired_items": expired_count,
            "valid_items": total_count - expired_count,
            "dynamic_cache_enabled": self.enable_dynamic_cache,
            "cache_duration": self.cache_duration,
        }
